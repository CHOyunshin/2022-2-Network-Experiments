{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test - CNN classifier\n",
        "\n",
        "Colab의 CUDA와 비교하기 위해서 생성해준 Py model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.backends import cudnn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import cv2\n",
        "import re\n",
        "import imageio.v2 as imageio\n",
        "import copy\n",
        "\n",
        "# from models.CustomModelExample import MyClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataSet(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_root):\n",
        "        self.data_root = data_root\n",
        "        self.data = []\n",
        "        self.y = []\n",
        "\n",
        "        self.DirFolder = os.listdir(self.data_root)\n",
        "    \n",
        "        start=datetime.now()\n",
        "        for FolderIndex in range(len(self.DirFolder)):\n",
        "            self.DirData = os.listdir(self.data_root + '/' + self.DirFolder[FolderIndex])\n",
        "            for DataIndex in range(len(self.DirData)):\n",
        "                img= imageio.imread(self.data_root + '/'  + self.DirFolder[FolderIndex] +'/'+  self.DirData[DataIndex])\n",
        "                numbers = int(re.sub(r'[^0-9]', '', self.DirFolder[FolderIndex]))\n",
        "                y1 = int(numbers)\n",
        "                self.data.append(img)\n",
        "                self.y.append(y1)\n",
        "            \n",
        "        self.data = torch.from_numpy(np.array(self.data))\n",
        "        self.y = torch.from_numpy(np.array(self.y))\n",
        "        print('Data reading time spent : ', datetime.now()-start)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MyClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 5, stride = 1, padding = 2)\n",
        "        self.layer2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
        "\n",
        "        self.linear1 = nn.Linear(2048, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.size()) # Check data size\n",
        "        x = F.max_pool2d(F.relu(self.layer1(x)), (2,2))\n",
        "        #print(x.size())\n",
        "        x = F.max_pool2d(F.relu(self.layer2(x)), (2,2))\n",
        "        #print(x.size())\n",
        "        x = torch.flatten(x,1)\n",
        "        #print(x.size())\n",
        "        x = F.relu(self.linear1(x))\n",
        "        #print(x.size())\n",
        "        x = self.linear2(x)\n",
        "        #print(x.size())\n",
        "        return x\n",
        "#net = MyClassifier()\n",
        "#print(net)\n",
        "\n",
        "#td = torch.rand(50,1,64,64)\n",
        "#out = net(td)\n",
        "#print(out.size())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    # # GPU Allocating\n",
        "    print('==> CPU/GPU allocating..')\n",
        "    print(torch.backends.mps.is_available())\n",
        "    # device = torch.device(\"cuda\")\n",
        "    device = torch.device(\"mps\")\n",
        "    print( 'Device : ', device)\n",
        "    # print ('Available devices : ', torch.device_count())\n",
        "    # print('Selecing GPU : ',torch.get_device_name(device))\n",
        "    total_t = datetime.now()\n",
        "\n",
        "    # Load Data\n",
        "    print('==> Dataset selecting..')\n",
        "    data_root = \"./labeled_data\" # Set your data root\n",
        "    data_root = \"./labeled_data_self\" # Set your data root\n",
        "    MyDataSet = CustomDataSet(data_root)\n",
        "\n",
        "    # Data Set Split\n",
        "    train_size = int(0.7 * len(MyDataSet)) # Training Size\n",
        "    valid_size = int(0.2 * len(MyDataSet)) # Validation Size\n",
        "    test_size = len(MyDataSet) - train_size - valid_size # Test Size\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(MyDataSet, [train_size, valid_size, test_size])\n",
        "\n",
        "    print('Number of train data : ', len(train_dataset))\n",
        "    print('Number of valid data : ', len(valid_dataset))\n",
        "    print('Number of test data : ', len(test_dataset))\n",
        "\n",
        "    # Model\n",
        "    print('==> Building model..')\n",
        "    nb_epochs = 30 # Set an epoch 1\n",
        "    torch.manual_seed(10) #Set your seed number\n",
        "    BatchSize = 50 # Set a batch size\n",
        "\n",
        "    resume = True #If you have the training networks, change this to True.\n",
        "    LoadPath = './save'\n",
        "    LoadEpoch = 29 #Set your best model epoch.\n",
        "    if resume == True: \n",
        "        LoadName = 'best_model_'+ str(LoadEpoch) + '.pth'\n",
        "        model = torch.load(os.path.join(LoadPath,LoadName))\n",
        "    else:\n",
        "        model = MyClassifier()\n",
        "        LoadEpoch = 0\n",
        "    \n",
        "    model = model.to(device)\n",
        "    print('Is resume : ', resume)\n",
        "    print('Number of epochs : {0:^4}, Batch size: {1}'.format(nb_epochs, BatchSize))\n",
        "\n",
        "    if device == 'cuda':\n",
        "        #model = torch.nn.DataParallel(model)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, num_workers=0)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader), eta_min=0, last_epoch=-1)\n",
        "\n",
        "    # Start the log\n",
        "    if os.path.isdir(LoadPath) == False:\n",
        "        os.mkdir(LoadPath)  \n",
        "\n",
        "    original_stdout = sys.stdout\n",
        "    if resume == False:\n",
        "        with open(LoadPath + '/result.txt','w') as f:\n",
        "            sys.stdout = f\n",
        "            print(sys._getframe().f_code.co_filename)\n",
        "            print(data_root)\n",
        "            print('start time : ', datetime.now())\n",
        "            sys.stdout = original_stdout\n",
        "\n",
        "    min_loss = 99999 # Allocate default value\n",
        "    print('==> Training model..')\n",
        "    for epoch in range(1,nb_epochs+1) :\n",
        "        #Train\n",
        "        start_epoch = datetime.now()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        valid_total = 0\n",
        "        valid_correct = 0\n",
        "        #label_total = list(0. for i in range(16))\n",
        "        #label_correct = list(0. for i in range(16))\n",
        "        for batch_idx, (datum, targets) in enumerate(train_dataloader):\n",
        "            start = datetime.now()\n",
        "            datum, targets= datum.to(device), targets.long().to(device)\n",
        "            img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "            bsz = targets.shape[0]\n",
        "\n",
        "            # Computing loss\n",
        "            out = model(img) \n",
        "            loss = criterion(out, targets)\n",
        "            \n",
        "            # Update model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "            print('Epoch: {0} | Progressing: {1} / {2} | Train loss: {3:0.6f} | Time spent: {4}, \\n'.format(epoch, (batch_idx)*BatchSize + datum[0].size(0), len(train_dataset), train_loss/(batch_idx+1), datetime.now()-start))\n",
        "            \n",
        "\n",
        "        #Valid\n",
        "        model.eval()\n",
        "        valid_loss_value = 0\n",
        "        with torch.no_grad():\n",
        "            for valid_batch_idx, (datum, targets) in enumerate(valid_dataloader):\n",
        "                datum, targets= datum.to(device), targets.to(device)\n",
        "                img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "                bsz = targets.shape[0]\n",
        "                #computing loss\n",
        "                out = model(img) \n",
        "                _, predicted = torch.max(out,1)\n",
        "                c = (predicted == targets).squeeze()\n",
        "                valid_total += targets.size(0)\n",
        "                valid_correct += (predicted == targets).sum().item()\n",
        "                #for i in range(bsz):\n",
        "                #    label = targets[i]\n",
        "                #    label_correct[label] += c[i].item()\n",
        "                #    label_total[label] += 1\n",
        "            \n",
        "                # compute loss\n",
        "                valid_loss = criterion(out, targets.long())\n",
        "                valid_loss_value += valid_loss.item()\n",
        "\n",
        "        # remember minimum loss model\n",
        "        is_min = valid_loss_value < min_loss\n",
        "        min_loss = min(valid_loss_value, min_loss)\n",
        "\n",
        "        if is_min == True:\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "\n",
        "        print('Epoch: {0} | Train loss: {1:0.6f}, Valid loss: {2:0.6f}, Valid Acc: {3} / {4} | Time spent: {5}'.format(epoch, train_loss/(batch_idx+1), valid_loss_value/(valid_batch_idx+1), valid_correct, valid_total, datetime.now()-start_epoch ))\n",
        "        # Save the log\n",
        "        with open(LoadPath+'/result.txt','a') as f:\n",
        "            sys.stdout = f\n",
        "            print('Epoch: {0:^4} | Train loss: {1:0.6f}, Valid loss: {2:0.6f}, Valid Acc: {3} / {4} | Time spent: {5}'.format(epoch, train_loss/(batch_idx+1), valid_loss_value/(valid_batch_idx+1), valid_correct, valid_total, datetime.now()-start_epoch ))\n",
        "            sys.stdout = original_stdout\n",
        "            \n",
        "\n",
        "    #Test\n",
        "    print('==> Testing the model..')\n",
        "    best_model.eval()\n",
        "    test_loss_value = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    label_total = list(0. for i in range(16))\n",
        "    label_correct = list(0. for i in range(16))\n",
        "    with torch.no_grad():\n",
        "        for test_batch_idx, (datum, targets) in enumerate(test_dataloader):\n",
        "            datum, targets= datum.to(device), targets.to(device)\n",
        "            img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "            bsz = targets.shape[0]\n",
        "            #computing loss\n",
        "            out = best_model(img) \n",
        "            _, predicted = torch.max(out,1)\n",
        "            c = (predicted == targets).squeeze()\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            for i in range(bsz):\n",
        "                label = targets[i]\n",
        "                label_correct[label] += c[i].item()\n",
        "                label_total[label] += 1\n",
        "        \n",
        "            # compute loss\n",
        "            test_loss = criterion(out, targets.long())\n",
        "            test_loss_value += test_loss.item()\n",
        "    \n",
        "\n",
        "    # Save the best model\n",
        "    SaveName = 'best_model_'+ str(LoadEpoch + best_epoch) + '.pth'\n",
        "    torch.save(best_model, os.path.join(LoadPath,SaveName))\n",
        "\n",
        "    print('Best epoch : ', best_epoch)     \n",
        "    for i in range(16):\n",
        "        print('Accuracy of label {0} : {1:0.0f} / {2:0.0f}'.format(i, label_correct[i], label_total[i]))\n",
        "\n",
        "    print('Test Accuracy : {0:0.3f}, {1} / {2}'.format(correct/total * 100, correct, total ) )\n",
        "    print()\n",
        "    print('Total Time spent with {1}: {0}'.format(datetime.now() - total_t, device))\n",
        "\n",
        "    # Save the log\n",
        "    with open(LoadPath+'/result.txt','a') as f:\n",
        "        sys.stdout = f\n",
        "        print('Best epoch : ', best_epoch)    \n",
        "        for i in range(16):\n",
        "            print('Accuracy of label {0} : {1:0.0f} / {2:0.0f}'.format(i, label_correct[i], label_total[i]))\n",
        "        print('Test Accuracy : {0:0.3f}%, ({1} / {2})'.format(correct/total * 100, correct, total ) )\n",
        "\n",
        "        sys.stdout = original_stdout\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.824"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "round(14 / 17, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyClassifier(\n",
            "  (layer1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (layer2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (linear1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (linear2): Linear(in_features=1024, out_features=16, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = torch.load('./save/best_model_29.pth')\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset의 Test \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Given Dataset으로 학습한 Model로 생성한 label 데이터 학습시키기 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Week2에서 만든 dataset으로 할습한 model을 이용해서 주어진 dataset을 이용해 실제 dataset 판단하기 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.load('./save/best_model_59.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data reading time spent :  0:00:00.570791\n",
            "==> Testing the model..\n",
            "Accuracy of label 1 : 136 / 200 -> 68.000\n",
            "Accuracy of label 4 : 197 / 200 -> 98.500\n",
            "Accuracy of label 5 : 152 / 200 -> 76.000\n",
            "Test Accuracy : 80.833, 485 / 600\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"mps\")\n",
        "Mydataset = CustomDataSet(\"./labeled_data\")\n",
        "Mydataloader = DataLoader(Mydataset, batch_size=len(Mydataset), shuffle= True, num_workers=0)\n",
        "\n",
        "#Test\n",
        "print('==> Testing the model..')\n",
        "model.eval()\n",
        "test_loss_value = 0\n",
        "total = 0\n",
        "correct = 0\n",
        "label_total = list(0. for i in range(16))\n",
        "label_correct = list(0. for i in range(16))\n",
        "with torch.no_grad():\n",
        "    for test_batch_idx, (datum, targets) in enumerate(Mydataloader):\n",
        "        datum, targets= datum.to(device), targets.to(device)\n",
        "        img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "        bsz = targets.shape[0]\n",
        "        #computing loss\n",
        "        out = model(img) \n",
        "        _, predicted = torch.max(out,1)\n",
        "        c = (predicted == targets).squeeze()\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        for i in range(bsz):\n",
        "            label = targets[i]\n",
        "            label_correct[label] += c[i].item()\n",
        "            label_total[label] += 1\n",
        "    total = 600\n",
        "        # compute loss\n",
        "        # test_loss = criterion(out, targets.long())\n",
        "        # test_loss_value += test_loss.item()\n",
        "label_radar = [1,4,5]\n",
        "for i in label_radar:\n",
        "    print('Accuracy of label {0} : {1:0.0f} / {2:0.0f} -> {3:0.3f}'.format(i, label_correct[i], label_total[i], label_correct[i]/label_total[i] * 100))       \n",
        "print('Test Accuracy : {0:0.3f}, {1} / {2}'.format(correct/total * 100, correct, total ) )\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.8.13 ('DL-torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3584091cda227b8e59fda59e5fdf3aec4997f3a2464c55243d7618073e2ad776"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
