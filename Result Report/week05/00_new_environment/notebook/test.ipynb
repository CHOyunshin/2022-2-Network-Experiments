{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test - CNN classifier\n",
        "Network Communication LAB Week5 USRP (3) Experiment\n",
        "\n",
        "Colab의 CUDA와 비교하기 위해서 생성해준 Py model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### import Library / Set the Data loader / Set Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.backends import cudnn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import cv2\n",
        "import re\n",
        "import imageio.v2 as imageio\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataSet(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_root):\n",
        "        self.data_root = data_root\n",
        "        self.data = []\n",
        "        self.y = []\n",
        "\n",
        "        self.DirFolder = os.listdir(self.data_root)\n",
        "    \n",
        "        start=datetime.now()\n",
        "        for FolderIndex in range(len(self.DirFolder)):\n",
        "            self.DirData = os.listdir(self.data_root + '/' + self.DirFolder[FolderIndex])\n",
        "            for DataIndex in range(len(self.DirData)):\n",
        "                img= imageio.imread(self.data_root + '/'  + self.DirFolder[FolderIndex] +'/'+  self.DirData[DataIndex])\n",
        "                numbers = int(re.sub(r'[^0-9]', '', self.DirFolder[FolderIndex]))\n",
        "                y1 = int(numbers)\n",
        "                self.data.append(img)\n",
        "                self.y.append(y1)\n",
        "            \n",
        "        self.data = torch.from_numpy(np.array(self.data))\n",
        "        self.y = torch.from_numpy(np.array(self.y))\n",
        "        print('Data reading time spent : ', datetime.now()-start)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class MyClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyClassifier,self).__init__()\n",
        "        self.keep_prob = 0.5\n",
        "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
        "        #    Conv     -> (?, 28, 28, 32)\n",
        "        #    Pool     -> (?, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
        "        #    Conv      ->(?, 14, 14, 64)\n",
        "        #    Pool      ->(?, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L3 ImgIn shape=(?, 7, 7, 64)\n",
        "        #    Conv      ->(?, 7, 7, 128)\n",
        "        #    Pool      ->(?, 4, 4, 128)\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
        "\n",
        "        # L4 FC 4x4x128 inputs -> 625 outputs\n",
        "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.layer4 = torch.nn.Sequential(\n",
        "            self.fc1,\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=1 - self.keep_prob))\n",
        "        # L5 Final FC 625 inputs -> 12 outputs\n",
        "        self.fc2 = torch.nn.Linear(625, 12, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
        "        out = self.layer4(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = MyClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyClassifier(\n",
            "  (layer1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (layer2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (linear1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (linear2): Linear(in_features=1024, out_features=16, bias=True)\n",
            ")\n",
            "torch.Size([50, 16])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MyClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 5, stride = 1, padding = 2)\n",
        "        self.layer2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.linear1 = nn.Linear(2048, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.size()) # Check data size\n",
        "        x = F.max_pool2d(F.relu(self.layer1(x)), (2,2))\n",
        "        #print(x.size())\n",
        "        x = F.max_pool2d(F.relu(self.layer2(x)), (2,2))\n",
        "        #print(x.size())\n",
        "        x = torch.flatten(x,1)\n",
        "        #print(x.size())\n",
        "        x = F.relu(self.linear1(x))\n",
        "        #print(x.size())\n",
        "        x = self.linear2(x)\n",
        "        #print(x.size())\n",
        "        return x\n",
        "\n",
        "model = MyClassifier()\n",
        "\n",
        "print(model)\n",
        "\n",
        "td = torch.rand(50,1,64,64)\n",
        "out = model(td)\n",
        "print(out.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyClassifier(\n",
            "  (layer1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (layer2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (layer3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (linear1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (linear2): Linear(in_features=1024, out_features=16, bias=True)\n",
            ")\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (50x256 and 4096x1024)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [5], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m     34\u001b[0m td \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m50\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m64\u001b[39m,\u001b[39m64\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m out \u001b[39m=\u001b[39m model(td)\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39msize())\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn [5], line 24\u001b[0m, in \u001b[0;36mMyClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x,\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39m#print(x.size())\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(x))\n\u001b[1;32m     25\u001b[0m \u001b[39m#print(x.size())\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50x256 and 4096x1024)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MyClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 5, stride = 1, padding = 2)\n",
        "        self.layer2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.linear1 = nn.Linear(4096, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.size()) # Check data size\n",
        "        x = F.max_pool2d(F.relu(self.layer1(x)), (2,2))\n",
        "        #print(x.size())\n",
        "        x = F.max_pool2d(F.relu(self.layer2(x)), (2,2))\n",
        "        #print(x.size())\n",
        "        x = F.max_pool2d(F.relu(self.layer3(x)), (2,2))\n",
        "        x = torch.flatten(x,1)\n",
        "        #print(x.size())\n",
        "        x = F.relu(self.linear1(x))\n",
        "        #print(x.size())\n",
        "        x = self.linear2(x)\n",
        "        #print(x.size())\n",
        "        return x\n",
        "\n",
        "model = MyClassifier()\n",
        "\n",
        "print(model)\n",
        "\n",
        "td = torch.rand(50,1,64,64)\n",
        "out = model(td)\n",
        "print(out.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'graph.png'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchviz import make_dot\n",
        "from torch.autograd import Variable\n",
        "td = torch.rand(50,1,64,64)\n",
        "make_dot(model(td), params = dict(model.named_parameters())).render('graph', format='png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 64, 64]             416\n",
            "            Conv2d-2           [-1, 32, 16, 16]           4,640\n",
            "            Linear-3                 [-1, 1024]       2,098,176\n",
            "            Linear-4                   [-1, 16]          16,400\n",
            "================================================================\n",
            "Total params: 2,119,632\n",
            "Trainable params: 2,119,632\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 0.57\n",
            "Params size (MB): 8.09\n",
            "Estimated Total Size (MB): 8.67\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "summary(model,input_size=(1,64,64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 01 \n",
        "---\n",
        "* given label dataset과 given model을 이용해서 실험 \n",
        "* 총 10회를 반복해서 그중에서 best model 의 pth 로 저장하고 저장된 모델을 이용해서 앞서 test  모델의 실험다음으로 진행하는  \n",
        "생성한 label dataset에 대해서도 test를 진행하고자 한다.\n",
        "\n",
        "\n",
        "* Input data : labeled_data \n",
        "* Output : Save Experiment 01 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.419625\n",
            "Number of train data :  1680\n",
            "Number of valid data :  480\n",
            "Number of test data :  240\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # # GPU Allocating\n",
        "    print('==> CPU/GPU allocating..')\n",
        "    print(torch.backends.mps.is_available())\n",
        "    # device = torch.device(\"cuda\")\n",
        "    device = torch.device(\"mps\")\n",
        "    print( 'Device : ', device)\n",
        "    # print ('Available devices : ', torch.device_count())\n",
        "    # print('Selecing GPU : ',torch.get_device_name(device))\n",
        "    total_t = datetime.now()\n",
        "\n",
        "    # Load Data\n",
        "    print('==> Dataset selecting..')\n",
        "    data_root = \"./labeled_data\" # Set your data root\n",
        "    # data_root = \"./labeled_data_self\" # Set your data root\n",
        "    MyDataSet = CustomDataSet(data_root)\n",
        "\n",
        "    # Data Set Split\n",
        "    train_size = int(0.7 * len(MyDataSet)) # Training Size\n",
        "    valid_size = int(0.2 * len(MyDataSet)) # Validation Size\n",
        "    test_size = len(MyDataSet) - train_size - valid_size # Test Size\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(MyDataSet, [train_size, valid_size, test_size])\n",
        "\n",
        "    print('Number of train data : ', len(train_dataset))\n",
        "    print('Number of valid data : ', len(valid_dataset))\n",
        "    print('Number of test data : ', len(test_dataset))\n",
        "\n",
        "    # Model\n",
        "    print('==> Building model..')\n",
        "    nb_epochs = 30 # Set an epoch 1\n",
        "    torch.manual_seed(10) #Set your seed number\n",
        "    BatchSize = 50 # Set a batch size\n",
        "\n",
        "    resume = False #If you have the training networks, change this to True.\n",
        "    LoadPath = './save'\n",
        "    LoadEpoch = 14 #Set your best model epoch.\n",
        "    if resume == True: \n",
        "        LoadName = 'best_model_'+ str(LoadEpoch) + '.pth'\n",
        "        model = torch.load(os.path.join(LoadPath,LoadName))\n",
        "    else:\n",
        "        model = MyClassifier()\n",
        "        LoadEpoch = 0\n",
        "    \n",
        "    model = model.to(device)\n",
        "    print('Is resume : ', resume)\n",
        "    print('Number of epochs : {0}, Batch size: {1}'.format(nb_epochs, BatchSize))\n",
        "\n",
        "    if device == 'cuda':\n",
        "        #model = torch.nn.DataParallel(model)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, num_workers=0)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader), eta_min=0, last_epoch=-1)\n",
        "\n",
        "    # Start the log\n",
        "    if os.path.isdir(LoadPath) == False:\n",
        "        os.mkdir(LoadPath)\n",
        "\n",
        "    original_stdout = sys.stdout\n",
        "    if resume == False:\n",
        "        with open(LoadPath+'/result_Experiment_01_#'+ str(repeat_test)+'.txt','w') as f:\n",
        "        # with open(LoadPath + '/result.txt','w') as f:\n",
        "            sys.stdout = f\n",
        "            print(sys._getframe().f_code.co_filename)\n",
        "            print(data_root)\n",
        "            print('start time : ', datetime.now())\n",
        "            sys.stdout = original_stdout\n",
        "\n",
        "    min_loss = 99999 # Allocate default value\n",
        "    print('==> Training model..')\n",
        "    for epoch in range(1,nb_epochs+1) :\n",
        "        #Train\n",
        "        start_epoch = datetime.now()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        valid_total = 0\n",
        "        valid_correct = 0\n",
        "        #label_total = list(0. for i in range(16))\n",
        "        #label_correct = list(0. for i in range(16))\n",
        "        for batch_idx, (datum, targets) in enumerate(train_dataloader):\n",
        "            start = datetime.now()\n",
        "            datum, targets= datum.to(device), targets.long().to(device)\n",
        "            img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "            bsz = targets.shape[0]\n",
        "\n",
        "            # Computing loss\n",
        "            out = model(img) \n",
        "            loss = criterion(out, targets)\n",
        "            \n",
        "            # Update model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            #print('Epoch: {0} | Progressing: {1} / {2} | Train loss: {3:0.6f} | Time spent: {4}, \\n'.format(epoch, (batch_idx)*BatchSize + datum[0].size(0), len(train_dataset), train_loss/(batch_idx+1), datetime.now()-start))\n",
        "            \n",
        "\n",
        "        #Valid\n",
        "        model.eval()\n",
        "        valid_loss_value = 0\n",
        "        with torch.no_grad():\n",
        "            for valid_batch_idx, (datum, targets) in enumerate(valid_dataloader):\n",
        "                datum, targets= datum.to(device), targets.to(device)\n",
        "                img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "                bsz = targets.shape[0]\n",
        "                #computing loss\n",
        "                out = model(img) \n",
        "                _, predicted = torch.max(out,1)\n",
        "                c = (predicted == targets).squeeze()\n",
        "                valid_total += targets.size(0)\n",
        "                valid_correct += (predicted == targets).sum().item()\n",
        "                #for i in range(bsz):\n",
        "                #    label = targets[i]\n",
        "                #    label_correct[label] += c[i].item()\n",
        "                #    label_total[label] += 1\n",
        "            \n",
        "                # compute loss\n",
        "                valid_loss = criterion(out, targets.long())\n",
        "                valid_loss_value += valid_loss.item()\n",
        "\n",
        "        # remember minimum loss model\n",
        "        is_min = valid_loss_value < min_loss\n",
        "        min_loss = min(valid_loss_value, min_loss)\n",
        "\n",
        "        if is_min == True:\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "\n",
        "        print('Epoch: {0} | Train loss: {1:0.6f}, Valid loss: {2:0.6f}, Valid Acc: {3} / {4} | Time spent: {5}'.format(epoch, train_loss/(batch_idx+1), valid_loss_value/(valid_batch_idx+1), valid_correct, valid_total, datetime.now()-start_epoch ))\n",
        "        # Save the log\n",
        "        with open(LoadPath+'/result_Experiment_01_#'+ str(repeat_test)+'.txt','a') as f:\n",
        "        # with open(LoadPath+'/result.txt','a') as f:\n",
        "            sys.stdout = f\n",
        "            print('Epoch: {0} | Train loss: {1:0.6f}, Valid loss: {2:0.6f}, Valid Acc: {3} / {4} | Time spent: {5}'.format(epoch, train_loss/(batch_idx+1), valid_loss_value/(valid_batch_idx+1), valid_correct, valid_total, datetime.now()-start_epoch ))\n",
        "            sys.stdout = original_stdout\n",
        "            \n",
        "\n",
        "    #Test\n",
        "    print('==> Testing the model..')\n",
        "    best_model.eval()\n",
        "    test_loss_value = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    label_total = list(0. for i in range(16))\n",
        "    label_correct = list(0. for i in range(16))\n",
        "    with torch.no_grad():\n",
        "        for test_batch_idx, (datum, targets) in enumerate(test_dataloader):\n",
        "            datum, targets= datum.to(device), targets.to(device)\n",
        "            img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "            bsz = targets.shape[0]\n",
        "            #computing loss\n",
        "            out = best_model(img) \n",
        "            _, predicted = torch.max(out,1)\n",
        "            c = (predicted == targets).squeeze()\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            for i in range(bsz):\n",
        "                label = targets[i]\n",
        "                label_correct[label] += c[i].item()\n",
        "                label_total[label] += 1\n",
        "        \n",
        "            # compute loss\n",
        "            test_loss = criterion(out, targets.long())\n",
        "            test_loss_value += test_loss.item()\n",
        "    \n",
        "\n",
        "    # Save the best model\n",
        "    SaveName = 'best_model_'+ str(LoadEpoch + best_epoch) + '.pth'\n",
        "    label_total = list(0. for i in range(16))\n",
        "    torch.save(best_model, os.path.join(LoadPath,SaveName))\n",
        "\n",
        "    print('Best epoch : ', best_epoch)    \n",
        "    for i in range(14):\n",
        "        print('Accuracy of label {0} : {1:0.0f} / {2:0.0f}'.format(i, label_correct[i], label_total[i]))\n",
        "\n",
        "    print('Test Accuracy : {0:0.3f}, {1} / {2}'.format(correct/total * 100, correct, total ) )\n",
        "\n",
        "    # Save the log\n",
        "    with open(LoadPath+'/result_Experiment_01_#'+ str(repeat_test)+'.txt','a') as f:\n",
        "        sys.stdout = f\n",
        "        print('Best epoch : ', best_epoch)    \n",
        "        for i in range(16):\n",
        "            print('Accuracy of label {0} : {1:0.0f} / {2:0.0f}'.format(i, label_correct[i], label_total[i]))\n",
        "        print('Test Accuracy : {0:0.3f}%, ({1} / {2})'.format(correct/total * 100, correct, total ) )\n",
        "\n",
        "        result_accuracy = round(correct / total * 100, 3)\n",
        "\n",
        "        sys.stdout = original_stdout\n",
        "    \n",
        "    # return result_accuracy, correct, total\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Result_accuracy = []\n",
        "    # Result_correct = []\n",
        "    # Result_total = []\n",
        "    for repeat_test in range(1):\n",
        "        main()\n",
        "    #     Result_accuracy.append(main()[0])\n",
        "    #     Result_correct.append(main()[1])\n",
        "    #     Result_total.append(main()[2])\n",
        "    \n",
        "    # for i in range(10):\n",
        "    #     print('Test Accuracy : {0:0.3f}%, ({1} / {2})'.format(Result_correct[i]/Result_total[i] * 100, Result_correct[i], Result_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Take Setting in Tensor Board\n",
        "\n",
        "[이거 참고해서 만드는중](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Tensor Board Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer, Criterion 설정해 주기 \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader), eta_min=0, last_epoch=-1)\n",
        "\n",
        "writer = SummaryWriter('runs/experiment_01')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Loader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.707463\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "nb_epochs = 30 # Set an epoch 1\n",
        "torch.manual_seed(10) #Set your seed number\n",
        "BatchSize = 50 # Set a batch size\n",
        "\n",
        "# Load Data\n",
        "print('==> Dataset selecting..')\n",
        "data_root = \"./labeled_data\" # Set your data root\n",
        "# data_root = \"./labeled_data_self\" # Set your data root\n",
        "MyDataSet = CustomDataSet(data_root)\n",
        "\n",
        "# Data Set Split\n",
        "train_size = int(0.7 * len(MyDataSet)) # Training Size\n",
        "valid_size = int(0.2 * len(MyDataSet)) # Validation Size\n",
        "test_size = len(MyDataSet) - train_size - valid_size # Test Size\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(MyDataSet, [train_size, valid_size, test_size])\n",
        "\n",
        "# Dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Writing to TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper function to show an image\n",
        "# (used in the `plot_classes_preds` function below)\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get some random training images\n",
        "import matplotlib\n",
        "from more_itertools import one\n",
        "\n",
        "dataiter = iter(train_dataloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images = images.reshape([-1, 1, 64, 64]).float()\n",
        "# Create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "#show images\n",
        "# matplotlib_imshow(img_grid, one_channel= True)\n",
        "\n",
        "# Write to Tensorboard\n",
        "writer.add_image('Spectogram item', img_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow installation not found - running with reduced feature set.\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "I1011 18:15:59.153190 6222524416 plugin.py:429] Monitor runs begin\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.10.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!tensorboard --logdir=runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow installation not found - running with reduced feature set.\n",
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "00_new_environment/notebook/runs/experiment_01\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) ^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/bin/tensorboard\", line 10, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/tensorboard/main.py\", line 46, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/tensorboard/program.py\", line 276, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/tensorboard/uploader/uploader_subcommand.py\", line 692, in run\n",
            "    return _run(flags, self._experiment_url_callback)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/tensorboard/uploader/uploader_subcommand.py\", line 94, in _run\n",
            "    _prompt_for_user_ack(intent)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-torch/lib/python3.8/site-packages/tensorboard/uploader/uploader_subcommand.py\", line 67, in _prompt_for_user_ack\n",
            "    response = input(\"Continue? (yes/NO) \")\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "tensorboard dev upload --logdir {'00_new_environment/notebook/runs/experiment_01'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer.add_graph(model, images)\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
          ]
        }
      ],
      "source": [
        "# helper function\n",
        "def select_n_random(data, labels, n=100):\n",
        "    '''\n",
        "    Selects n random datapoints and their corresponding labels from a dataset\n",
        "    '''\n",
        "    assert len(data) == len(labels)\n",
        "\n",
        "    perm = torch.randperm(len(data))\n",
        "    return data[perm][:n], labels[perm][:n]\n",
        "\n",
        "# select random images and their target indices\n",
        "images, labels = select_n_random(MyDataSet.data, MyDataSet.y)\n",
        "\n",
        "classes = list(0. for i in range(16))\n",
        "\n",
        "# get the class labels for each image\n",
        "class_labels = [classes[lab] for lab in labels]\n",
        "# log embeddings\n",
        "features = images.view(-1, 64 * 64)\n",
        "writer.add_embedding(features,\n",
        "                    metadata=class_labels,\n",
        "                    label_img=images.unsqueeze(1))\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 03\n",
        "---\n",
        "* week4 실험으로부터 생성한 spectogram을 이용해서 만들어준 label dataset과 given model을 이용해서 실험 \n",
        "* 총 10회를 반복해서 그중에서 best model 의 pth 로 저장하고 저장된 모델을 이용해서 앞서 test  모델의 실험다음으로 진행하는  \n",
        "생성한 label dataset에 대해서도 test를 진행하고자 한다.\n",
        "\n",
        "\n",
        "* Input data : labeled_data_self \n",
        "* Output : Save Experiment 03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.740929\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.858083\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.404371\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.392128\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.370998\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.371688\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.380390\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.389887\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.381369\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.377641\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.385300\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.369471\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.365745\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.376150\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.374120\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.391785\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.391424\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.373634\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.368951\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.366077\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.369214\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.372862\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.367214\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.372242\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.373515\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.390470\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.379618\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.362765\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.464991\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.458787\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.437442\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.731367\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.414381\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.414298\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.431227\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.415164\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.396647\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.393404\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.386068\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.383504\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.372910\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.374498\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.382536\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.398428\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.378268\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.396202\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.377775\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.402078\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.507903\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.465367\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.464620\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.452931\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.465721\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.495922\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.394701\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.420993\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.418430\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.457839\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.484672\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.475437\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.416615\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.412948\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.721526\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.377128\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.413932\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.401666\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.384539\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.362682\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.379225\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.372699\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.360940\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.363805\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.370404\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.401834\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.421999\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.407042\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.394108\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.385108\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.393411\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.369843\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.380140\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.376732\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.372409\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.365971\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.379517\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.379632\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.365690\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.384036\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.409380\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.423259\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.387232\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.402778\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.401843\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.669676\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.372607\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.374517\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.389825\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.386587\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.358695\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.358275\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.385678\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.404765\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.394821\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.375939\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.374769\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.377393\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.373423\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.389420\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.370030\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.369098\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.361890\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.358791\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.357903\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.357206\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.359493\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.367813\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.377179\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.407930\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.381433\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.368913\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.479826\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.446884\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.439011\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.386357\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.645603\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.455980\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.392980\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.403978\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.423605\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.446682\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.427120\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.452915\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.432350\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.398229\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.382054\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.367712\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.367222\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.397833\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.374113\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.365289\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.380865\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.377859\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.368488\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.364879\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.360376\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.364427\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.361005\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.365750\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.363745\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.367709\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.366296\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.363058\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.362739\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.366001\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.360959\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.539301\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.372975\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.373570\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.381363\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.395017\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.373776\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.368383\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.363764\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.363376\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.361290\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.360520\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.363340\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.361936\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.360744\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.365119\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.364802\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.358743\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.363553\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.361089\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.360041\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.361046\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.368102\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.370009\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.362509\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.372978\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.392704\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.369712\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.362393\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.363729\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.362187\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.367189\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.548950\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.370819\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.373206\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.378065\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.382602\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.359462\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.363421\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.362973\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.361583\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.360646\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.365125\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.363797\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.367539\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.361424\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.362640\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.364670\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.365451\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.375498\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.375936\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.393837\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.374035\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.367479\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.365529\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.362337\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.362047\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.363376\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.359218\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.357214\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.359060\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.361182\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.358449\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.533712\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.371833\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.378346\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.385569\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.367134\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.362744\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.361432\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.360064\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.361626\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.367963\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.362597\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.359506\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.361259\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.362095\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.359706\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.360776\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.359320\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.364186\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.366157\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.379507\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.391490\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.378197\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.393854\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.398245\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.371497\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.377361\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.375098\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.374183\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.372751\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.369382\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.367415\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.535956\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.379564\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.398624\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.401499\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.400423\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.386585\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.381023\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.367091\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.385297\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.368193\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.371868\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.382783\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.370859\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.369886\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.375905\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.388126\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.380216\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.380528\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.369746\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.386591\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.396715\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.390619\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.412163\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.388615\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.385384\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.377097\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.372203\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.386722\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.387659\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.389199\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.383280\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n",
            "==> CPU/GPU allocating..\n",
            "True\n",
            "Device :  mps\n",
            "==> Dataset selecting..\n",
            "Data reading time spent :  0:00:00.541291\n",
            "Number of train data :  2116\n",
            "Number of valid data :  604\n",
            "Number of test data :  304\n",
            "==> Building model..\n",
            "Is resume :  False\n",
            "Number of epochs : 30, Batch size: 50\n",
            "==> Training model..\n",
            "Epoch: 1 | Train loss: inf, Valid loss: 2.275521, Valid Acc: 136 / 604 | Time spent: 0:00:00.398152\n",
            "Epoch: 2 | Train loss: 1.647866, Valid loss: 0.748470, Valid Acc: 435 / 604 | Time spent: 0:00:00.380087\n",
            "Epoch: 3 | Train loss: 0.577784, Valid loss: 0.429748, Valid Acc: 500 / 604 | Time spent: 0:00:00.396717\n",
            "Epoch: 4 | Train loss: 0.366743, Valid loss: 0.493703, Valid Acc: 497 / 604 | Time spent: 0:00:00.390002\n",
            "Epoch: 5 | Train loss: 0.306355, Valid loss: 0.317051, Valid Acc: 531 / 604 | Time spent: 0:00:00.374395\n",
            "Epoch: 6 | Train loss: 0.185503, Valid loss: 0.378676, Valid Acc: 526 / 604 | Time spent: 0:00:00.377232\n",
            "Epoch: 7 | Train loss: 0.187899, Valid loss: 0.274455, Valid Acc: 539 / 604 | Time spent: 0:00:00.371125\n",
            "Epoch: 8 | Train loss: 0.110298, Valid loss: 0.346672, Valid Acc: 527 / 604 | Time spent: 0:00:00.377303\n",
            "Epoch: 9 | Train loss: 0.119150, Valid loss: 0.229254, Valid Acc: 543 / 604 | Time spent: 0:00:00.374745\n",
            "Epoch: 10 | Train loss: 0.065528, Valid loss: 0.318449, Valid Acc: 536 / 604 | Time spent: 0:00:00.373676\n",
            "Epoch: 11 | Train loss: 0.093159, Valid loss: 0.250005, Valid Acc: 543 / 604 | Time spent: 0:00:00.366076\n",
            "Epoch: 12 | Train loss: 0.042922, Valid loss: 0.332812, Valid Acc: 533 / 604 | Time spent: 0:00:00.376085\n",
            "Epoch: 13 | Train loss: 0.066063, Valid loss: 0.343151, Valid Acc: 544 / 604 | Time spent: 0:00:00.374635\n",
            "Epoch: 14 | Train loss: 0.032409, Valid loss: 0.293986, Valid Acc: 547 / 604 | Time spent: 0:00:00.371990\n",
            "Epoch: 15 | Train loss: 0.037467, Valid loss: 0.297618, Valid Acc: 548 / 604 | Time spent: 0:00:00.366115\n",
            "Epoch: 16 | Train loss: 0.018282, Valid loss: 0.330741, Valid Acc: 549 / 604 | Time spent: 0:00:00.366965\n",
            "Epoch: 17 | Train loss: 0.042193, Valid loss: 0.271208, Valid Acc: 544 / 604 | Time spent: 0:00:00.375221\n",
            "Epoch: 18 | Train loss: 0.017284, Valid loss: 0.295203, Valid Acc: 540 / 604 | Time spent: 0:00:00.360569\n",
            "Epoch: 19 | Train loss: 0.017758, Valid loss: 0.367799, Valid Acc: 549 / 604 | Time spent: 0:00:00.367684\n",
            "Epoch: 20 | Train loss: 0.006313, Valid loss: 0.407041, Valid Acc: 554 / 604 | Time spent: 0:00:00.380272\n",
            "Epoch: 21 | Train loss: 0.003895, Valid loss: 0.312919, Valid Acc: 553 / 604 | Time spent: 0:00:00.387601\n",
            "Epoch: 22 | Train loss: 0.002182, Valid loss: 0.338822, Valid Acc: 549 / 604 | Time spent: 0:00:00.388536\n",
            "Epoch: 23 | Train loss: 0.001302, Valid loss: 0.318145, Valid Acc: 550 / 604 | Time spent: 0:00:00.396552\n",
            "Epoch: 24 | Train loss: 0.000990, Valid loss: 0.339277, Valid Acc: 551 / 604 | Time spent: 0:00:00.372521\n",
            "Epoch: 25 | Train loss: 0.000843, Valid loss: 0.358269, Valid Acc: 551 / 604 | Time spent: 0:00:00.362315\n",
            "Epoch: 26 | Train loss: 0.000718, Valid loss: 0.340115, Valid Acc: 549 / 604 | Time spent: 0:00:00.359700\n",
            "Epoch: 27 | Train loss: 0.000637, Valid loss: 0.355504, Valid Acc: 551 / 604 | Time spent: 0:00:00.360870\n",
            "Epoch: 28 | Train loss: 0.000546, Valid loss: 0.360706, Valid Acc: 550 / 604 | Time spent: 0:00:00.364181\n",
            "Epoch: 29 | Train loss: 0.000501, Valid loss: 0.357269, Valid Acc: 551 / 604 | Time spent: 0:00:00.357575\n",
            "Epoch: 30 | Train loss: 0.000427, Valid loss: 0.400350, Valid Acc: 550 / 604 | Time spent: 0:00:00.359790\n",
            "==> Testing the model..\n",
            "Best epoch :  9\n",
            "Accuracy of label 0 : 15 / 15\n",
            "Accuracy of label 1 : 33 / 33\n",
            "Accuracy of label 2 : 15 / 18\n",
            "Accuracy of label 3 : 19 / 32\n",
            "Accuracy of label 4 : 30 / 31\n",
            "Accuracy of label 5 : 51 / 52\n",
            "Accuracy of label 6 : 17 / 18\n",
            "Accuracy of label 7 : 14 / 21\n",
            "Accuracy of label 8 : 19 / 21\n",
            "Accuracy of label 9 : 15 / 16\n",
            "Accuracy of label 10 : 0 / 0\n",
            "Accuracy of label 11 : 0 / 0\n",
            "Accuracy of label 12 : 23 / 27\n",
            "Accuracy of label 13 : 18 / 20\n",
            "Test Accuracy : 88.487, 269 / 304\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # # GPU Allocating\n",
        "    print('==> CPU/GPU allocating..')\n",
        "    print(torch.backends.mps.is_available())\n",
        "    # device = torch.device(\"cuda\")\n",
        "    device = torch.device(\"mps\")\n",
        "    print( 'Device : ', device)\n",
        "    # print ('Available devices : ', torch.device_count())\n",
        "    # print('Selecing GPU : ',torch.get_device_name(device))\n",
        "    total_t = datetime.now()\n",
        "\n",
        "    # Load Data\n",
        "    print('==> Dataset selecting..')\n",
        "    # data_root = \"./labeled_data\" # Set your data root\n",
        "    data_root = \"./labeled_data_self\" # Set your data root\n",
        "    MyDataSet = CustomDataSet(data_root)\n",
        "\n",
        "    # Data Set Split\n",
        "    train_size = int(0.7 * len(MyDataSet)) # Training Size\n",
        "    valid_size = int(0.2 * len(MyDataSet)) # Validation Size\n",
        "    test_size = len(MyDataSet) - train_size - valid_size # Test Size\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(MyDataSet, [train_size, valid_size, test_size])\n",
        "\n",
        "    print('Number of train data : ', len(train_dataset))\n",
        "    print('Number of valid data : ', len(valid_dataset))\n",
        "    print('Number of test data : ', len(test_dataset))\n",
        "\n",
        "    # Model\n",
        "    print('==> Building model..')\n",
        "    nb_epochs = 30 # Set an epoch 1\n",
        "    torch.manual_seed(10) #Set your seed number\n",
        "    BatchSize = 50 # Set a batch size\n",
        "\n",
        "    resume = False #If you have the training networks, change this to True.\n",
        "    LoadPath = './save'\n",
        "    LoadEpoch = 14 #Set your best model epoch.\n",
        "    if resume == True: \n",
        "        LoadName = 'best_model_'+ str(LoadEpoch) + '.pth'\n",
        "        model = torch.load(os.path.join(LoadPath,LoadName))\n",
        "    else:\n",
        "        model = MyClassifier()\n",
        "        LoadEpoch = 0\n",
        "    \n",
        "    model = model.to(device)\n",
        "    print('Is resume : ', resume)\n",
        "    print('Number of epochs : {0}, Batch size: {1}'.format(nb_epochs, BatchSize))\n",
        "\n",
        "    if device == 'cuda':\n",
        "        #model = torch.nn.DataParallel(model)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, num_workers=0)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader), eta_min=0,\n",
        "                                                               last_epoch=-1)\n",
        "\n",
        "    # Start the log\n",
        "    if os.path.isdir(LoadPath) == False:\n",
        "        os.mkdir(LoadPath)\n",
        "\n",
        "    original_stdout = sys.stdout\n",
        "    if resume == False:\n",
        "        with open(LoadPath+'/result_Experiment_01_#'+ str(repeat_test)+'.txt','w') as f:\n",
        "        # with open(LoadPath + '/result.txt','w') as f:\n",
        "            sys.stdout = f\n",
        "            print(sys._getframe().f_code.co_filename)\n",
        "            print(data_root)\n",
        "            print('start time : ', datetime.now())\n",
        "            sys.stdout = original_stdout\n",
        "\n",
        "    min_loss = 99999 # Allocate default value\n",
        "    print('==> Training model..')\n",
        "    for epoch in range(1,nb_epochs+1) :\n",
        "        #Train\n",
        "        start_epoch = datetime.now()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        valid_total = 0\n",
        "        valid_correct = 0\n",
        "        #label_total = list(0. for i in range(16))\n",
        "        #label_correct = list(0. for i in range(16))\n",
        "        for batch_idx, (datum, targets) in enumerate(train_dataloader):\n",
        "            start = datetime.now()\n",
        "            datum, targets= datum.to(device), targets.long().to(device)\n",
        "            img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "            bsz = targets.shape[0]\n",
        "\n",
        "            # Computing loss\n",
        "            out = model(img) \n",
        "            loss = criterion(out, targets)\n",
        "            \n",
        "            # Update model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            #print('Epoch: {0} | Progressing: {1} / {2} | Train loss: {3:0.6f} | Time spent: {4}, \\n'.format(epoch, (batch_idx)*BatchSize + datum[0].size(0), len(train_dataset), train_loss/(batch_idx+1), datetime.now()-start))\n",
        "            \n",
        "\n",
        "        #Valid\n",
        "        model.eval()\n",
        "        valid_loss_value = 0\n",
        "        with torch.no_grad():\n",
        "            for valid_batch_idx, (datum, targets) in enumerate(valid_dataloader):\n",
        "                datum, targets= datum.to(device), targets.to(device)\n",
        "                img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "                bsz = targets.shape[0]\n",
        "                #computing loss\n",
        "                out = model(img) \n",
        "                _, predicted = torch.max(out,1)\n",
        "                c = (predicted == targets).squeeze()\n",
        "                valid_total += targets.size(0)\n",
        "                valid_correct += (predicted == targets).sum().item()\n",
        "                #for i in range(bsz):\n",
        "                #    label = targets[i]\n",
        "                #    label_correct[label] += c[i].item()\n",
        "                #    label_total[label] += 1\n",
        "            \n",
        "                # compute loss\n",
        "                valid_loss = criterion(out, targets.long())\n",
        "                valid_loss_value += valid_loss.item()\n",
        "\n",
        "        # remember minimum loss model\n",
        "        is_min = valid_loss_value < min_loss\n",
        "        min_loss = min(valid_loss_value, min_loss)\n",
        "\n",
        "        if is_min == True:\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "\n",
        "        print('Epoch: {0} | Train loss: {1:0.6f}, Valid loss: {2:0.6f}, Valid Acc: {3} / {4} | Time spent: {5}'.format(epoch, train_loss/(batch_idx+1), valid_loss_value/(valid_batch_idx+1), valid_correct, valid_total, datetime.now()-start_epoch ))\n",
        "        # Save the log\n",
        "        with open(LoadPath+'/result_Experiment_01_#'+ str(repeat_test)+'.txt','a') as f:\n",
        "        # with open(LoadPath+'/result.txt','a') as f:\n",
        "            sys.stdout = f\n",
        "            print('Epoch: {0} | Train loss: {1:0.6f}, Valid loss: {2:0.6f}, Valid Acc: {3} / {4} | Time spent: {5}'.format(epoch, train_loss/(batch_idx+1), valid_loss_value/(valid_batch_idx+1), valid_correct, valid_total, datetime.now()-start_epoch ))\n",
        "            sys.stdout = original_stdout\n",
        "            \n",
        "\n",
        "    #Test\n",
        "    print('==> Testing the model..')\n",
        "    best_model.eval()\n",
        "    test_loss_value = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    label_total = list(0. for i in range(16))\n",
        "    label_correct = list(0. for i in range(16))\n",
        "    with torch.no_grad():\n",
        "        for test_batch_idx, (datum, targets) in enumerate(test_dataloader):\n",
        "            datum, targets= datum.to(device), targets.to(device)\n",
        "            img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "            bsz = targets.shape[0]\n",
        "            #computing loss\n",
        "            out = best_model(img) \n",
        "            _, predicted = torch.max(out,1)\n",
        "            c = (predicted == targets).squeeze()\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            for i in range(bsz):\n",
        "                label = targets[i]\n",
        "                label_correct[label] += c[i].item()\n",
        "                label_total[label] += 1\n",
        "        \n",
        "            # compute loss\n",
        "            test_loss = criterion(out, targets.long())\n",
        "            test_loss_value += test_loss.item()\n",
        "    \n",
        "\n",
        "    # Save the best model\n",
        "    SaveName = 'best_model_'+ str(LoadEpoch + best_epoch) + '.pth'\n",
        "    torch.save(best_model, os.path.join(LoadPath,SaveName))\n",
        "\n",
        "    print('Best epoch : ', best_epoch)    \n",
        "    for i in range(14):\n",
        "        print('Accuracy of label {0} : {1:0.0f} / {2:0.0f}'.format(i, label_correct[i], label_total[i]))\n",
        "\n",
        "    print('Test Accuracy : {0:0.3f}, {1} / {2}'.format(correct/total * 100, correct, total ) )\n",
        "\n",
        "    # Save the log\n",
        "    with open(LoadPath+'/result_Experiment_01_#'+ str(repeat_test)+'.txt','a') as f:\n",
        "        sys.stdout = f\n",
        "        print('Best epoch : ', best_epoch)    \n",
        "        for i in range(16):\n",
        "            print('Accuracy of label {0} : {1:0.0f} / {2:0.0f}'.format(i, label_correct[i], label_total[i]))\n",
        "        print('Test Accuracy : {0:0.3f}%, ({1} / {2})'.format(correct/total * 100, correct, total ) )\n",
        "\n",
        "        sys.stdout = original_stdout\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for repeat_test in range(10):\n",
        "        main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus - 01 General Data Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 01  Model : Given Dataset (labeled_data) --> Test : Custom Dataset (labeled_data2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.load('./save_Experiment_01/best_model_Experiment_01.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data reading time spent :  0:00:00.719603\n",
            "==> Testing the model..\n",
            "Accuracy of label 1 : 373 / 374 -> 99.733\n",
            "Accuracy of label 4 : 372 / 375 -> 99.200\n",
            "Accuracy of label 5 : 472 / 475 -> 99.368\n",
            "Test Accuracy of label 1,4,5 : 99.428, 1217 / 1224\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"mps\")\n",
        "Mydataset = CustomDataSet(\"./labeled_data2/\")\n",
        "Mydataloader = DataLoader(Mydataset, batch_size=len(Mydataset), shuffle= True, num_workers=0)\n",
        "\n",
        "#Test\n",
        "print('==> Testing the model..')\n",
        "model.eval()\n",
        "test_loss_value = 0\n",
        "total = 0\n",
        "correct = 0\n",
        "label_total = list(0. for i in range(16))\n",
        "label_correct = list(0. for i in range(16))\n",
        "with torch.no_grad():\n",
        "    for test_batch_idx, (datum, targets) in enumerate(Mydataloader):\n",
        "        datum, targets= datum.to(device), targets.to(device)\n",
        "        img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "        bsz = targets.shape[0]\n",
        "        #computing loss\n",
        "        out = model(img) \n",
        "        _, predicted = torch.max(out,1)\n",
        "        c = (predicted == targets).squeeze()\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        for i in range(bsz):\n",
        "            label = targets[i]\n",
        "            label_correct[label] += c[i].item()\n",
        "            label_total[label] += 1\n",
        "        # compute loss\n",
        "        # test_loss = criterion(out, targets.long())\n",
        "        # test_loss_value += test_loss.item()\n",
        "\n",
        "# Test for label 1, 4, 5 only\n",
        "label_145 = [1,4,5]\n",
        "correct_145, total_145 = 0,0\n",
        "# for i in range(16):\n",
        "for i in label_145:\n",
        "    try:    \n",
        "        correct_145 += label_correct[i]\n",
        "        total_145 += label_total[i]\n",
        "        print('Accuracy of label {0} : {1:0.0f} / {2:0.0f} -> {3:0.3f}'.format(i, label_correct[i], label_total[i], label_correct[i]/label_total[i] * 100))\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Zero Division Error -> since there are no images in label folder\")\n",
        "print('Test Accuracy of label 1,4,5 : {0:0.3f}, {1:0.0f} / {2:0.0f}'.format(correct_145/total_145 * 100, correct_145, total_145 ) )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 02  Model : Custom Dataset (labeled_data2)  --> Test : Given Dataset (labeled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.load('./save_Experiment_03/best_model_Experiment_03.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data reading time spent :  0:00:00.614359\n",
            "==> Testing the model..\n",
            "Accuracy of label 1 : 197 / 200 -> 98.500\n",
            "Accuracy of label 4 : 197 / 200 -> 98.500\n",
            "Accuracy of label 5 : 193 / 200 -> 96.500\n",
            "Test Accuracy of label 1,4,5 : 97.833, 587 / 600\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"mps\")\n",
        "Mydataset = CustomDataSet(\"./labeled_data/\")\n",
        "Mydataloader = DataLoader(Mydataset, batch_size=len(Mydataset), shuffle= True, num_workers=0)\n",
        "\n",
        "#Test\n",
        "print('==> Testing the model..')\n",
        "model.eval()\n",
        "test_loss_value = 0\n",
        "total = 0\n",
        "correct = 0\n",
        "label_total = list(0. for i in range(16))\n",
        "label_correct = list(0. for i in range(16))\n",
        "with torch.no_grad():\n",
        "    for test_batch_idx, (datum, targets) in enumerate(Mydataloader):\n",
        "        datum, targets= datum.to(device), targets.to(device)\n",
        "        img = datum.reshape([-1, 1, 64, 64]).float()\n",
        "        bsz = targets.shape[0]\n",
        "        #computing loss\n",
        "        out = model(img) \n",
        "        _, predicted = torch.max(out,1)\n",
        "\n",
        "        c = (predicted == targets).squeeze()\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        for i in range(bsz):\n",
        "            label = targets[i]\n",
        "            label_correct[label] += c[i].item()\n",
        "            label_total[label] += 1\n",
        "        # compute loss\n",
        "        # test_loss = criterion(out, targets.long())\n",
        "        # test_loss_value += test_loss.item()\n",
        "# Test for label 1, 4, 5 only\n",
        "label_145 = [1,4,5]\n",
        "correct_145, total_145 = 0,0\n",
        "# for i in range(16):\n",
        "for i in label_145:\n",
        "    try:    \n",
        "        correct_145 += label_correct[i]\n",
        "        total_145 += label_total[i]\n",
        "        print('Accuracy of label {0} : {1:0.0f} / {2:0.0f} -> {3:0.3f}'.format(i, label_correct[i], label_total[i], label_correct[i]/label_total[i] * 100))\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Zero Division Error -> since there are no images in label folder\")\n",
        "print('Test Accuracy of label 1,4,5 : {0:0.3f}, {1:0.0f} / {2:0.0f}'.format(correct_145/total_145 * 100, correct_145, total_145 ) )"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.8.13 ('DL-torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3584091cda227b8e59fda59e5fdf3aec4997f3a2464c55243d7618073e2ad776"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
